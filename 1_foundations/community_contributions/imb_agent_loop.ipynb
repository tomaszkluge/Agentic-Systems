{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719c5614",
      "metadata": {},
      "outputs": [],
      "source": [
        "from rich.console import Console\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd47d313",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "openai = OpenAI()\n",
        "groq = OpenAI(api_key=os.getenv(\"GROQ_API_KEY\"), base_url=\"https://api.groq.com/openai/v1\")\n",
        "claude = anthropic.Anthropic()\n",
        "ollama = OpenAI(api_key=os.getenv(\"OLLAMA_API_KEY\"), base_url=\"http://localhost:11434/v1\")\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c432d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show(text):\n",
        "    try:\n",
        "        console.print(text)\n",
        "    except Exception:\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c9d96b",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts, scores = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b97aeb4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are PromptOptimizer, a tool-using agent that improves prompts through an iterative loop.\n",
        "Mission:\n",
        "- Given a bad_prompt, produce an improved prompt.\n",
        "- First, evaluate the given prompt.\n",
        "- Immediately after, rewrite the prompt.\n",
        "- Then, continue the loop.\n",
        "- You MUST run exactly 5 iterations, without counting the first one.\n",
        "- Each iteration is: REWRITE (Using Tools)→ EVALUATE (using tools) → IMPROVE.\n",
        "- Keep the user's original intent. Do not change the task, only how it is requested.\n",
        "\n",
        "Available tools:\n",
        "1) rewrite_prompt(input_prompt, score, feedback)\n",
        "   - Generates a candidate rewritten prompt.\n",
        "   - The parameters are: input_prompt, score, feedback.\n",
        "2) evaluate_prompt(prompt)\n",
        "   - Returns a checklist-based score and diagnostics.\n",
        "   - The only parameter is the prompt.\n",
        "3) select_best()\n",
        "   - Selects the best prompt across iterations.\n",
        "   - This does not receive any parameter.\n",
        "\n",
        "Hard rules:\n",
        "- You MUST use tools. Do not do the rewrite or scoring “in your head”.\n",
        "- If critical info is missing, make minimal assumptions.\n",
        "- Do not ask the user questions unless explicitly allowed.\n",
        "- Avoid vague language. Replace subjective words with measurable constraints.\n",
        "- Always specify the expected output format inside the final prompt.\n",
        "- Do not reveal chain-of-thought. Only output the rewrite and the score.\n",
        "\n",
        "Stop condition:\n",
        "- After exactly 5 iterations, call select_best() and output the best prompt.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3402c61c",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_prompt = \"\"\"I don't how how to start with AI. Help me\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ac733c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_scores_report() -> str:\n",
        "    result = \"\"\n",
        "    for index, (prompt, score) in enumerate(zip(prompts, scores)):\n",
        "        color = 'bold_red' if score < 60 else 'bold_yellow' if 60 <= score < 80 else 'bold_green'\n",
        "        if index == 0:\n",
        "            result += f\"Initial prompt: {user_prompt}\\n\"\n",
        "        else:\n",
        "            result += f\"Iteration {index}: \"\n",
        "            result += f\"New Prompt: {prompt}. -> \"\n",
        "        result += f\"[{color}]  Score: {score}[/{color}]\\n\\n\\n\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "648fcd03",
      "metadata": {},
      "outputs": [],
      "source": [
        "#For Claude I need to extract the first json object from the response in text format\n",
        "def extract_first_json_object(text: str) -> dict:\n",
        "    start = text.find(\"{\")\n",
        "    end = text.rfind(\"}\")\n",
        "\n",
        "    if start == -1 or end == -1 or end <= start:\n",
        "        raise ValueError(\"No JSON object found in the text\")\n",
        "\n",
        "    json_str = text[start:end+1]\n",
        "    return json.loads(json_str)\n",
        "\n",
        "claude_activated = True\n",
        "\n",
        "def evaluate_prompt(prompt: str) -> dict:\n",
        "    system_prompt= f\"\"\"You are an expert Prompt Evaluator (Prompt Critic).\n",
        "\n",
        "Your job is to evaluate the quality of a given prompt.\n",
        "You must be strict, practical, and specific.\n",
        "\n",
        "You will receive:\n",
        "- prompt: the prompt to evaluate\n",
        "\n",
        "Your evaluation must judge whether the prompt, as written, would reliably produce high-quality outputs from an LLM.\n",
        "\n",
        "Rules:\n",
        "1) DO NOT rewrite the prompt.\n",
        "2) DO NOT invent external context.\n",
        "3) Evaluate ONLY what is explicitly present in the prompt.\n",
        "4) If critical information is missing, list it in the feedback.\n",
        "5) Do not show step-by-step reasoning. Provide only clear conclusions.\n",
        "6) You must evaluate between 1 and 100 where below 60 is bad. Between 60 and 80 is so so. Above 80 is excellent.\n",
        "\n",
        "Evaluation criteria: Objective & task definition, sufficient context, Output format specified, Quality criteria (definition of done), Ambiguity handling, Robustness / expected consistency, Safety / hallucination prevention (when relevant), Efficiency / signal-to-noise ratio\n",
        "\n",
        "Required output:\n",
        "Return ONLY valid JSON with this exact structure:\n",
        "    - prompt: the prompt that you are evaluating\n",
        "    - score: the score of the prompt\n",
        "    - feedback: the feedback of the prompt\n",
        "example: \n",
        "{{\n",
        "    \"prompt\": \"prompt\",\n",
        "    \"score\": 0,\n",
        "    \"feedback\": \"feedback\"\n",
        "}}\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"assistant\" if claude_activated else \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    \n",
        "    if claude_activated:\n",
        "        response = claude.messages.create(\n",
        "            model=\"claude-opus-4-6\",\n",
        "            max_tokens=1000,\n",
        "            messages=messages\n",
        "        )\n",
        "        content = extract_first_json_object(response.content[0].text)\n",
        "    else:\n",
        "        response = ollama.chat.completions.create(\n",
        "            model=\"deepseek-r1:1.5b\",\n",
        "            max_tokens=1000,\n",
        "            messages=messages,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        content = json.loads(response.choices[0].message.content)\n",
        "    \n",
        "    score = content.get(\"score\", content.get(\"Score\", 0))\n",
        "    prompts.append(prompt)\n",
        "    scores.append(score)\n",
        "    show(f\"I received the prompt: {prompt}. The score is: {score}\\n\\n\")\n",
        "    return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f05258",
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_best():\n",
        "    if len(scores) == 0:\n",
        "        return None\n",
        "    best_score, best_prompt = max(zip(scores, prompts), key=lambda x: x[0])\n",
        "    show(f\"The best prompt is: {best_prompt}. The score is: {best_score}\\n\\n\")\n",
        "    return {\n",
        "        \"best_prompt\": best_prompt,\n",
        "        \"score\": best_score,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216bde1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rewrite_prompt(prompt: str, score: str, feedback: str) -> str:\n",
        "    system_prompt = \"\"\"You are a prompt writer, you will receive a prompt, a score and a feedback.\n",
        "    You will rewrite the prompt to improve it by 2 points in the score.\n",
        "    You need to output a json with the following fields:\n",
        "    - new_prompt: the rewritten prompt\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Prompt: {prompt}\\nScore: {score}\\nFeedback: {feedback}\"}\n",
        "    ]\n",
        "    response = groq.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-20b\",\n",
        "        messages=messages,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    print(f\"I Have to rewrite this prompt: {prompt}. \\nThe score given by an evaluator is: {score}. \\nThe feedback provider is: {feedback}. \\n\\n\")\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37191e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_prompt_json = {\n",
        "    \"name\": \"evaluate_prompt\",\n",
        "    \"description\": \"Evaluate a prompt and return a json with the score and the feedback. Return the evaluation as structured JSON\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"prompt\": {\"type\": \"string\", \"description\": \"The prompt to evaluate\"}\n",
        "        },\n",
        "        \"required\": [\"prompt\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f286be02",
      "metadata": {},
      "outputs": [],
      "source": [
        "rewrite_prompt_json = {\n",
        "    \"name\": \"rewrite_prompt\",\n",
        "    \"description\": \"Rewrite a prompt to improve it\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"prompt\": {\"type\": \"string\", \"description\": \"The prompt to rewrite\"},\n",
        "            \"score\": {\"type\": \"string\", \"description\": \"The score of the prompt\"},\n",
        "            \"feedback\": {\"type\": \"string\", \"description\": \"The feedback of the prompt\"}\n",
        "        },\n",
        "        \"required\": [\"prompt\", \"score\", \"feedback\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3450e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "select_best_json = {\n",
        "    \"name\": \"select_best\",\n",
        "    \"description\": \"Select the best prompt from the history\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc253eae",
      "metadata": {},
      "outputs": [],
      "source": [
        "tools = [\n",
        "    {\"type\": \"function\", \"function\": evaluate_prompt_json}, \n",
        "    {\"type\": \"function\", \"function\": rewrite_prompt_json}, \n",
        "    {\"type\": \"function\", \"function\": select_best_json}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb525319",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_tool_calls(tool_calls):\n",
        "    results = []\n",
        "    for tool_call in tool_calls:\n",
        "        tool_name = tool_call.function.name\n",
        "        arguments = json.loads(tool_call.function.arguments)\n",
        "        tool = globals().get(tool_name)\n",
        "        result = tool(**arguments) if tool else {}\n",
        "        results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359a6290",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loop(messages):\n",
        "    done = False\n",
        "    while not done:\n",
        "        response = openai.chat.completions.create(model=\"gpt-5.2\", messages=messages, tools=tools, reasoning_effort=\"none\")\n",
        "        finish_reason = response.choices[0].finish_reason\n",
        "        if finish_reason==\"tool_calls\":\n",
        "            message = response.choices[0].message\n",
        "            tool_calls = message.tool_calls\n",
        "            results = handle_tool_calls(tool_calls)\n",
        "            messages.append(message)\n",
        "            messages.extend(results)\n",
        "        else:\n",
        "            done = True\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "loop(messages)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
