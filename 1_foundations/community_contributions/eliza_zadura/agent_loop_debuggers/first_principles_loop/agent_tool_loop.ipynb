{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9985df65",
      "metadata": {},
      "source": [
        "<table style=\"margin: 0; text-align: left; width:100%\">\n",
        "    <tr>\n",
        "        <td>\n",
        "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
        "            <span style=\"color:#ff7800;\">Now try to build an Agent Loop from scratch yourself!<br/>\n",
        "            Create a new .ipynb and make one from first principles, referring back to this as needed.<br/>\n",
        "            It's one of the few times that I recommend typing from scratch - it's a very satisfying result.\n",
        "            </span>\n",
        "            <p>\n",
        "            Read from the <code>docs_ez/first_principles_loop/buggy_kata</code> folder, which contains a collection of files with bugs in them. Parse test output from the terminal, and use it to fix the bugs. Rerun the tests until they all pass, or until hard stop.\n",
        "            </p>\n",
        "            <p>\n",
        "            To reset back to the original buggy state at any time, run:<br/>\n",
        "            <code>python buggy_kata/reset_kata.py</code>\n",
        "            </p>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cccc830",
      "metadata": {},
      "source": [
        "## Loop:\n",
        "\n",
        "**Observe**:\n",
        "- run tests\n",
        "\n",
        "**Select**:\n",
        "- parse failures\n",
        "- pick one failing test (or pick the first one)\n",
        "\n",
        " **Act**:\n",
        "- read the relevant file\n",
        "- apply the smallest change to fix that failure\n",
        "\n",
        " **Verify**:\n",
        "- run tests again\n",
        "- mark failure resolved or not\n",
        "\n",
        " **Terminate**:\n",
        "- all tests pass or\n",
        "- max iterations reached"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddfbc0f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with some imports - rich is a library for making formatted text output in the terminal\n",
        "\n",
        "from rich.console import Console\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "376d0301",
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_FOLDER = \"buggy_kata\"\n",
        "MAX_ITERATIONS = 15\n",
        "\n",
        "\n",
        "def reset_buggy_kata():\n",
        "    \"\"\"Reset buggy_kata by running the dedicated reset helper.\"\"\"\n",
        "    from buggy_kata.reset_kata import reset_buggy_kata_state\n",
        "\n",
        "    restored_file = reset_buggy_kata_state()\n",
        "    print(f\"âœ… Reset complete: {restored_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62ce13b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a single console instance for consistent output\n",
        "console = Console()\n",
        "\n",
        "\n",
        "def show(text):\n",
        "    \"\"\"Print formatted text using rich console.\"\"\"\n",
        "    try:\n",
        "        console.print(text)\n",
        "    except Exception:\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ede6b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af37cd71",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the workspace root (where the notebook is running from)\n",
        "WORKSPACE_ROOT = Path.cwd()\n",
        "\n",
        "# Debug: print where we think the workspace is\n",
        "print(f\"WORKSPACE_ROOT: {WORKSPACE_ROOT}\")\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "\n",
        "\n",
        "# tools:\n",
        "def run_tests(folder_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Run pytest on the tests folder within the target folder.\n",
        "    Returns combined stdout/stderr output.\n",
        "    \"\"\"\n",
        "    # Resolve to absolute path if relative\n",
        "    abs_path = Path(folder_path)\n",
        "    if not abs_path.is_absolute():\n",
        "        abs_path = WORKSPACE_ROOT / folder_path\n",
        "\n",
        "    # Use sys.executable to ensure we use the same Python as the notebook\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pytest\", \"tests/\", \"-v\"],\n",
        "        cwd=str(abs_path),\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "    output = result.stdout + result.stderr\n",
        "    return output\n",
        "\n",
        "\n",
        "def read_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Read and return the contents of a file.\n",
        "    \"\"\"\n",
        "    # Resolve to absolute path if relative\n",
        "    abs_path = Path(file_path)\n",
        "    if not abs_path.is_absolute():\n",
        "        abs_path = WORKSPACE_ROOT / file_path\n",
        "\n",
        "    with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def write_file(file_path: str, content: str) -> str:\n",
        "    \"\"\"\n",
        "    Write content to a file, overwriting any existing content.\n",
        "    Returns confirmation message.\n",
        "    \"\"\"\n",
        "    # Resolve to absolute path if relative\n",
        "    abs_path = Path(file_path)\n",
        "    if not abs_path.is_absolute():\n",
        "        abs_path = WORKSPACE_ROOT / file_path\n",
        "\n",
        "    with open(abs_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    return f\"Successfully wrote to {file_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decb61e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# tool definitions\n",
        "\n",
        "run_tests_json = {\n",
        "    \"name\": \"run_tests\",\n",
        "    \"description\": \"Run pytest in buggy_kata/tests and return pass/fail output with tracebacks.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"folder_path\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Path to the folder containing the tests/ subdirectory\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"folder_path\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "read_file_json = {\n",
        "    \"name\": \"read_file\",\n",
        "    \"description\": \"Read and return file contents. For source code, prefer buggy_kata/src/utils.py.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"file_path\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Path to read. Use buggy_kata/src/utils.py for fixes and buggy_kata/tests/test_utils.py for context.\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"file_path\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "write_file_json = {\n",
        "    \"name\": \"write_file\",\n",
        "    \"description\": \"Write full content to a file. Only modify buggy_kata/src/utils.py.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"file_path\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Path to write. Use buggy_kata/src/utils.py.\",\n",
        "            },\n",
        "            \"content\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The complete content to write to the file\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"file_path\", \"content\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "tools = [\n",
        "    {\"type\": \"function\", \"function\": run_tests_json},\n",
        "    {\"type\": \"function\", \"function\": read_file_json},\n",
        "    {\"type\": \"function\", \"function\": write_file_json},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0eddfbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Regex to strip ANSI escape codes\n",
        "ANSI_ESCAPE = re.compile(r\"\\x1b\\[[0-9;]*m\")\n",
        "\n",
        "\n",
        "def strip_ansi(text: str) -> str:\n",
        "    \"\"\"Remove ANSI escape codes from text.\"\"\"\n",
        "    return ANSI_ESCAPE.sub(\"\", text)\n",
        "\n",
        "\n",
        "def summarize_test_output(output: str) -> str:\n",
        "    \"\"\"Extract a human-friendly summary from pytest output.\"\"\"\n",
        "    # Strip ANSI codes first!\n",
        "    clean = strip_ansi(output)\n",
        "    lines = clean.strip().split(\"\\n\")\n",
        "\n",
        "    # Find passed/failed counts and failed test names\n",
        "    failed_tests = []\n",
        "    passed_count = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    for line in lines:\n",
        "        # Look for the summary line like \"7 failed, 8 passed in 0.21s\"\n",
        "        if \" passed\" in line and (\"failed\" in line or \"==\" in line):\n",
        "            # Extract numbers\n",
        "            match = re.search(r\"(\\d+) passed\", line)\n",
        "            if match:\n",
        "                passed_count = int(match.group(1))\n",
        "            match = re.search(r\"(\\d+) failed\", line)\n",
        "            if match:\n",
        "                failed_count = int(match.group(1))\n",
        "\n",
        "        # Collect failed test names\n",
        "        if \"FAILED\" in line and \"::\" in line:\n",
        "            # Extract just the test function name\n",
        "            parts = line.split(\"::\")\n",
        "            if len(parts) >= 2:\n",
        "                test_name = parts[-1].split()[0].split(\"-\")[0]\n",
        "                if test_name not in failed_tests:\n",
        "                    failed_tests.append(test_name)\n",
        "\n",
        "    if failed_count > 0:\n",
        "        summary = f\"{failed_count} failed, {passed_count} passed\"\n",
        "        test_list = \", \".join(failed_tests[:4])\n",
        "        if len(failed_tests) > 4:\n",
        "            test_list += f\" (+{len(failed_tests) - 4} more)\"\n",
        "        return f\"âŒ {summary}\\n   Failed: {test_list}\"\n",
        "    elif passed_count > 0:\n",
        "        return f\"âœ… All {passed_count} tests passed!\"\n",
        "    else:\n",
        "        # Fallback - just show first few clean lines\n",
        "        preview = \"\\n\".join(lines[:3])\n",
        "        return preview if len(preview) < 200 else preview[:200] + \"...\"\n",
        "\n",
        "\n",
        "def report_tool_call(tool_name, arguments, result):\n",
        "    \"\"\"\n",
        "    Pretty-print what the agent is doing for each tool call.\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "\n",
        "    if tool_name == \"run_tests\":\n",
        "        console.print(\"\\n[bold cyan]ðŸ§ª Running tests...[/bold cyan]\")\n",
        "        console.print(f\"   [dim]folder:[/dim] {arguments.get('folder_path', 'N/A')}\")\n",
        "        # Print summary (already cleaned of ANSI codes)\n",
        "        summary = summarize_test_output(result)\n",
        "        for line in summary.split(\"\\n\"):\n",
        "            console.print(f\"   {line}\")\n",
        "\n",
        "    elif tool_name == \"read_file\":\n",
        "        path = arguments.get(\"file_path\", \"unknown\")\n",
        "        lines = result.count(\"\\n\") + 1\n",
        "        console.print(\n",
        "            f\"\\n[bold cyan]ðŸ“– Reading:[/bold cyan] {path} [dim]({lines} lines)[/dim]\"\n",
        "        )\n",
        "\n",
        "    elif tool_name == \"write_file\":\n",
        "        path = arguments.get(\"file_path\", \"unknown\")\n",
        "        content = arguments.get(\"content\", \"\")\n",
        "        console.print(\n",
        "            f\"\\n[bold cyan]âœï¸  Writing:[/bold cyan] {path} [dim]({len(content)} chars)[/dim]\"\n",
        "        )\n",
        "        console.print(\"   [green]âœ“ Saved[/green]\")\n",
        "\n",
        "    else:\n",
        "        console.print(f\"\\n[bold cyan]â–¶ {tool_name}[/bold cyan]\")\n",
        "        for key, value in arguments.items():\n",
        "            display = (\n",
        "                value[:80] + \"...\"\n",
        "                if isinstance(value, str) and len(value) > 80\n",
        "                else value\n",
        "            )\n",
        "            console.print(f\"   [dim]{key}:[/dim] {display}\")\n",
        "\n",
        "\n",
        "def handle_tool_calls(tool_calls):\n",
        "    \"\"\"\n",
        "    Execute each tool call and return results in the format expected by OpenAI.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for tool_call in tool_calls:\n",
        "        tool_name = tool_call.function.name\n",
        "        arguments = json.loads(tool_call.function.arguments)\n",
        "\n",
        "        # Look up the function by name and call it\n",
        "        tool = globals().get(tool_name)\n",
        "        result = tool(**arguments) if tool else f\"Unknown tool: {tool_name}\"\n",
        "\n",
        "        # Report what happened\n",
        "        report_tool_call(tool_name, arguments, result)\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": result if isinstance(result, str) else json.dumps(result),\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "            }\n",
        "        )\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf6a7dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loop(messages):\n",
        "    \"\"\"\n",
        "    The agent loop: call the model, handle tool calls, repeat until done or max iterations.\n",
        "    \"\"\"\n",
        "    iteration = 0\n",
        "    done = False\n",
        "    last_response_id = None\n",
        "\n",
        "    show(\"[bold magenta]ðŸ¤– Bug-Fixing Agent Started[/bold magenta]\")\n",
        "    show(f\"[dim]Target: {TARGET_FOLDER} | Max iterations: {MAX_ITERATIONS}[/dim]\\n\")\n",
        "\n",
        "    while not done and iteration < MAX_ITERATIONS:\n",
        "        iteration += 1\n",
        "        show(f\"[bold blue]â”â”â” Step {iteration}/{MAX_ITERATIONS} â”â”â”[/bold blue]\")\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            store=True,\n",
        "            metadata={\"run_mode\": \"with_trace\"},\n",
        "        )\n",
        "        last_response_id = response.id\n",
        "\n",
        "        finish_reason = response.choices[0].finish_reason\n",
        "        message = response.choices[0].message\n",
        "\n",
        "        if finish_reason == \"tool_calls\":\n",
        "            # Model wants to call tools\n",
        "            tool_calls = message.tool_calls\n",
        "\n",
        "            # Execute tools and get results\n",
        "            results = handle_tool_calls(tool_calls)\n",
        "\n",
        "            # Add assistant message and tool results to conversation\n",
        "            messages.append(message)\n",
        "            messages.extend(results)\n",
        "        else:\n",
        "            # Model is done (finish_reason == \"stop\")\n",
        "            done = True\n",
        "            show(\"\\n[bold green]âœ… Agent Complete![/bold green]\")\n",
        "            show(f\"[dim]Finished in {iteration} steps[/dim]\\n\")\n",
        "            if message.content:\n",
        "                show(\"[bold]Summary:[/bold]\")\n",
        "                show(message.content)\n",
        "\n",
        "            # Surface trace/log lookup details at the end of each run\n",
        "            if last_response_id:\n",
        "                show(f\"[dim]Trace ID: {last_response_id}[/dim]\")\n",
        "                show(\n",
        "                    f\"[dim]View trace: https://platform.openai.com/logs?api=chat-completions&id={last_response_id}[/dim]\"\n",
        "                )\n",
        "            else:\n",
        "                show(\"[dim]View traces: https://platform.openai.com/logs?api=chat-completions[/dim]\")\n",
        "\n",
        "    if iteration >= MAX_ITERATIONS:\n",
        "        show(f\"\\n[bold red]âš ï¸  Reached max iterations ({MAX_ITERATIONS})[/bold red]\")\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7dd17a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from rich.panel import Panel\n",
        "from rich.text import Text\n",
        "from rich.table import Table\n",
        "\n",
        "\n",
        "def format_conversation(messages, show_system=False):\n",
        "    \"\"\"\n",
        "    Display a human-readable summary of the agent conversation.\n",
        "\n",
        "    Args:\n",
        "        messages: The messages list from the agent loop\n",
        "        show_system: Whether to show the system prompt (default False)\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "\n",
        "    for msg in messages:\n",
        "        # Handle dict messages (user, system, tool results)\n",
        "        if isinstance(msg, dict):\n",
        "            role = msg.get(\"role\", \"unknown\")\n",
        "            content = msg.get(\"content\", \"\")\n",
        "\n",
        "            if role == \"system\":\n",
        "                if show_system:\n",
        "                    console.print(\n",
        "                        Panel(\n",
        "                            content[:300] + \"...\" if len(content) > 300 else content,\n",
        "                            title=\"[bold blue]System[/bold blue]\",\n",
        "                            border_style=\"blue\",\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            elif role == \"user\":\n",
        "                console.print(\n",
        "                    Panel(\n",
        "                        content,\n",
        "                        title=\"[bold green]User[/bold green]\",\n",
        "                        border_style=\"green\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            elif role == \"tool\":\n",
        "                # Tool results - show a compact summary\n",
        "                clean = strip_ansi(content)\n",
        "                if \"passed\" in clean or \"failed\" in clean:\n",
        "                    # Test output - show summary only\n",
        "                    summary = summarize_test_output(content)\n",
        "                    console.print(\n",
        "                        f\"   [dim]Tool result:[/dim] {summary.split(chr(10))[0]}\"\n",
        "                    )\n",
        "                elif len(clean) > 150:\n",
        "                    console.print(f\"   [dim]Tool result:[/dim] ({len(clean)} chars)\")\n",
        "                else:\n",
        "                    console.print(f\"   [dim]Tool result:[/dim] {clean[:100]}\")\n",
        "\n",
        "        # Handle ChatCompletionMessage objects (assistant responses)\n",
        "        elif hasattr(msg, \"role\") and msg.role == \"assistant\":\n",
        "            if msg.tool_calls:\n",
        "                # Show tool calls in a compact format\n",
        "                calls = [\n",
        "                    f\"{tc.function.name}({list(json.loads(tc.function.arguments).values())[0] if tc.function.arguments != '{}' else ''})\"\n",
        "                    for tc in msg.tool_calls\n",
        "                ]\n",
        "                console.print(\n",
        "                    f\"\\n[bold yellow]ðŸ¤– Agent:[/bold yellow] {', '.join(calls)}\"\n",
        "                )\n",
        "            elif msg.content:\n",
        "                console.print(\n",
        "                    Panel(\n",
        "                        msg.content,\n",
        "                        title=\"[bold yellow]ðŸ¤– Agent[/bold yellow]\",\n",
        "                        border_style=\"yellow\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "\n",
        "def show_summary(messages):\n",
        "    \"\"\"Show a quick stats summary of the conversation.\"\"\"\n",
        "    console = Console()\n",
        "\n",
        "    tool_counts = {}\n",
        "    for msg in messages:\n",
        "        if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "            for tc in msg.tool_calls:\n",
        "                name = tc.function.name\n",
        "                tool_counts[name] = tool_counts.get(name, 0) + 1\n",
        "\n",
        "    table = Table(title=\"Agent Run Summary\", show_header=True)\n",
        "    table.add_column(\"Tool\", style=\"cyan\")\n",
        "    table.add_column(\"Calls\", style=\"green\", justify=\"right\")\n",
        "\n",
        "    for tool, count in sorted(tool_counts.items()):\n",
        "        table.add_row(tool, str(count))\n",
        "\n",
        "    table.add_row(\"[bold]Total[/bold]\", f\"[bold]{sum(tool_counts.values())}[/bold]\")\n",
        "    console.print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a1d08d",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = f\"\"\"\n",
        "You are given a buggy kata. Fix failing tests with minimal edits.\n",
        "\n",
        "Target folder: {TARGET_FOLDER}\n",
        "\n",
        "Important constraints:\n",
        "- Run tests from {TARGET_FOLDER}.\n",
        "- Read tests from {TARGET_FOLDER}/tests/test_utils.py when needed.\n",
        "- Only edit {TARGET_FOLDER}/src/utils.py.\n",
        "- Do not edit files outside {TARGET_FOLDER}/src/utils.py.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Please fix all failing tests with trace. Start by running tests, then only edit buggy_kata/src/utils.py.\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9169bc37",
      "metadata": {},
      "source": [
        "system_message = f\"\"\"\n",
        "You are a bug-fixing agent. Your goal is to fix all failing tests in the codebase.\n",
        "\n",
        "Target folder: {TARGET_FOLDER}\n",
        "\n",
        "Your workflow:\n",
        "1. Run the tests to see what's failing\n",
        "2. Read the relevant source file to understand the bug\n",
        "3. Write the corrected file to fix the bug\n",
        "4. Repeat until all tests pass\n",
        "\n",
        "Important:\n",
        "- Fix one bug at a time, then re-run tests to verify\n",
        "- Make minimal changes - only fix what's broken\n",
        "- Read tests from {TARGET_FOLDER}/tests/test_utils.py for debugging context\n",
        "- Only modify {TARGET_FOLDER}/src/utils.py\n",
        "- Do not modify any other files\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Please fix all failing tests with trace. Start by running tests, then only edit buggy_kata/src/utils.py.\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0388df2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the agent loop!\n",
        "result = loop(messages)\n",
        "\n",
        "# Suppress the raw messages output by assigning to a variable\n",
        "# To see a formatted conversation history, run: format_conversation(result)\n",
        "# To see stats, run: show_summary(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0131735b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: View a formatted conversation summary\n",
        "format_conversation(result)\n",
        "\n",
        "# Optional: View tool usage stats\n",
        "show_summary(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abb97547",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset command (no uncommenting needed):\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.run([sys.executable, \"buggy_kata/reset_kata.py\"], check=True)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
